# Theory-of-neural-networks

## Approximation

### Classical results before deep learning

- [A. Pinkus. Approximation theory of the MLP model in neural networks](http://www2.math.technion.ac.il/~pinkus/papers/acta.pdf)
  - A good summary.
 
- [A. R. Barron. Universal Approximation bounds for superpositions of a sigmoidal function](https://ieeexplore.ieee.org/document/256500)
  - Bound obtained by probability method, independent of dimension.
  
- [Y. Makovoz. Random approximants and neural networks](https://www.sciencedirect.com/science/article/pii/S0021904596900313)
  - Bound obtained by probability method.
  
- [R. A. DeVore, R. Howard, and C. Micchelli. Optimal nonlinear approximation](https://link.springer.com/article/10.1007/BF01171759)
  - Famous nonlinear width, not directly related to neural networks.
  
- [V. Maiorov and J. Ratsaby. On the degree of approximation by manifolds of finite pseudo-dimension](https://link.springer.com/article/10.1007/s003659900108)
  - Nonlinear width based on pseudo-dimension, interesting and insightful, but not well-known.




























































